# 周报  

LWGAnet论文的复现




<img width="800" height="480" alt="df1f89c5c394172dda1a3a1eb41a787f" src="https://github.com/user-attachments/assets/a5fec696-b8f1-45c4-8bd7-ee8a536b95e4" />
<img width="800" height="480" alt="9c613fdf2ede64f096af0a0d9e797b4f" src="https://github.com/user-attachments/assets/cfb4f73a-6e6e-4072-8b5d-58ce2fffd8ba" />

自己跑的对比论文官方的精确率普遍会低一些，主要原因应该是园论文是训练了300个训练循环，我自己跑的是训练了50个训练循环，训练轮数不足导致模型未充分收敛


然后对跑出的数据差值进行排序

<img width="1000" height="500" alt="b93d068b872098a49dee7f34e6c3fafa" src="https://github.com/user-attachments/assets/c5c30da9-01b4-47b4-a84e-144f438e5448" />
<img width="1000" height="500" alt="cdc3415310c4eca3d01f5f5f8b74a061" src="https://github.com/user-attachments/assets/4e6dbead-5902-4649-87bf-d34b22d4d816" />


可看到虽然其他模型50轮训练可能还未收敛，但是LWGANet 系列在50轮训练后基本收敛，与300轮差值很小

模型结构对收敛速度的影响：

EdgeNexT 系列未收敛最严重（差值 12%-17%），可能因其轻量级设计（如 EdgeNexT XXS 仅 1.17M 参数）需要更多轮次才能充分学习特征；


CNN 类模型整体未收敛程度中等偏严重（差值 6%-8%），传统卷积对数据特征的学习需要更多迭代；

Transformer/Hybrid 类收敛速度介于两者之间，注意力机制帮助模型更快捕捉关键特征，但轻量级版本（如 EdgeViT XXS）仍需更多轮次。

LWGANet 系列（Hybrid）收敛速度最快（差值仅 1%-2%），说明其结构设计（如注意力机制、特征融合方式）更高效，能在少轮次下快速逼近最优解；

参数规模与收敛的关系：

参数越少的模型差值越大，说明小参数量模型需要更多轮次才能弥补容量不足，充分拟合数据；

参数量较大的模型差值更小，模型容量足够时，50 轮已能学到大部分核心特征。


以下是换了一个数据集后的结果，可以看到LWGANet 系列也是在50轮就接近于收敛，与之前的结论基本一致


这是NWPU数据集的结果

<img width="1205" height="692" alt="image" src="https://github.com/user-attachments/assets/75977bd6-4689-4dc5-84c8-a45df8ad7e44" />


这是UCM数据集的结果

<img width="1245" height="601" alt="image" src="https://github.com/user-attachments/assets/4cbc1c8a-f771-4d72-a5a9-4bbb5c136fdc" />
<img width="1244" height="427" alt="image" src="https://github.com/user-attachments/assets/c4429aba-235b-4517-a77c-5c2d43ea023b" />






